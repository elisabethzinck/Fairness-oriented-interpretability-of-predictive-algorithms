Sender: LSF System <lsfadmin@hpc.dtu.dk>
Subject: Job 11160387: <adam_wd=1e-2> in cluster <dcc> Done

Job <adam_wd=1e-2> was submitted from host <n-62-27-22> by user <s164175> in cluster <dcc> at Thu Nov 25 16:23:05 2021
Job was executed on host(s) <8*n-62-20-14>, in queue <gpuv100>, as user <s164175> in cluster <dcc> at Thu Nov 25 19:29:56 2021
</zhome/95/1/117606> was used as the home directory.
</zhome/95/1/117606/Documents/Fairness-oriented-interpretability-of-predictive-algorithms> was used as the working directory.
Started at Thu Nov 25 19:29:56 2021
Terminated at Thu Nov 25 23:23:04 2021
Results reported at Thu Nov 25 23:23:04 2021

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/sh 
### General options 
### -- specify queue -- 
#BSUB -q gpuv100
### -- set the job Name --  (should be the same as in script)
#BSUB -J adam_wd=1e-2
### -- ask for number of cores (default: 1) -- 
#BSUB -n 8 
### --- ask for gpu ---
#BSUB -gpu "num=1"
### -- specify that the cores must be on the same host -- 
#BSUB -R "span[hosts=1]"
### -- amount of memory per core/slot -- 
#BSUB -R "rusage[mem=10GB]"
### -- job gets killed if it exceeds xGB per core/slot -- 
#BSUB -M 10GB
### -- set walltime limit: hh:mm -- 
#BSUB -W 20:00
### -- Specify the output and error file. %J is the job-id -- 
### -- -o and -e mean append, -oo and -eo mean overwrite -- 
#BSUB -oo logs/adam_wd=1e-2/%J.out 

echo "Starting bash script"

### Make logging directory
mkdir -p logs/adam_wd=1e-2/

### Activate environment
module load python3/3.8.11
source .venv/bin/activate

### Run python script
echo "----------------------------"
echo "--- Output from Python -----"
echo "----------------------------"

# Running neural network training parsing 
# model_name, weight_decay, drop_out, extented_image_augmentation
python3 src/models/cheXpert_neural_network_w_argparser.py adam_wd=1e-2 1e-2 0 False

echo "----------------------------"
echo "---       DONE :)      -----"
echo "----------------------------"


------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   47948.32 sec.
    Max Memory :                                 5614 MB
    Average Memory :                             4946.35 MB
    Total Requested Memory :                     81920.00 MB
    Delta Memory :                               76306.00 MB
    Max Swap :                                   1 MB
    Max Processes :                              18
    Max Threads :                                65
    Run time :                                   13987 sec.
    Turnaround time :                            25199 sec.

The output (if any) follows:

Starting bash script
Loaded dependency [python3/3.8.11]: gcc/10.3.0-binutils-2.36.1
Loaded module: python3/3.8.11

Loading python3/3.8.11
  Loading requirement: gcc/10.3.0-binutils-2.36.1
----------------------------
--- Output from Python -----
----------------------------
Global seed set to 42
Using cache found in /zhome/95/1/117606/.cache/torch/hub/pytorch_vision_v0.10.0
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name        | Type     | Params
-----------------------------------------
0 | model       | DenseNet | 7.0 M 
1 | train_auroc | AUROC    | 0     
2 | val_auroc   | AUROC    | 0     
-----------------------------------------
14.3 K    Trainable params
7.0 M     Non-trainable params
7.0 M     Total params
27.873    Total estimated model params size (MB)
model_name: adam_wd=1e-2
wd: 0.01
dropout: 0.0
Extended Image Augmentation: True
--- Initializing model and datamodule ---
--- Setup training ---
--- Training model ---
Params to learn:
	 classifier.weight
	 classifier.bias
Global seed set to 42
Total time to run script: 232.38 mins
----------------------------
---       DONE :)      -----
----------------------------
